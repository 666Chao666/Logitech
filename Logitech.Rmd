---
title: "Logitech Interview"
author: "Chao Zhang"
date: "08/06/2020"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup and data prep, echo = FALSE, message=FALSE, warning=FALSE}
# load packages
library(tidyverse)
library(lubridate)
library(ggplot2)
library(forecast)
library(knitr)

# set default knitr chunks
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	fig.height = 4,
	fig.pos = "H",
	fig.width = 6,
	message = FALSE,
	warning = FALSE,
	cache = FALSE
)

# source functions
source("categoryConstruction.R")
source("dataFrameTransformation.R")

# load data
df <- read.csv('use_case.csv')

```

## preprocessing and data prep
This section focuses on preprocessing and cleaning the data. Some 
justifications on the decisions made are provided in the comments.
```{r EDA, echo = FALSE, message=FALSE, warning=FALSE}
# some basic data info
glimpse(df)

############################# missing values ###################################
# any missing values
sum(is.na(df))

# Yet manual inspection indicates multiple Null's
# try checking data types
sapply(df, class)

# locate rows with missing values with exact match of 'NULL' character string
nulls <- df %>%
  filter((df$Category1 == 'NULL' | df$Category2 == 'NULL' |
          df$Category3 == 'NULL' | is.na(.)))

############################# some discussion ##################################
# While the 'NULL' character strings don't necessarily have a direct impact on 
# time series analysis, missing values might indicate potential data quality 
# issues and due to the lack of extra information on the data-set removing 
# them is a safer option
############################# some discussion ##################################

# remove all rows with Null's
df <- df %>%
  filter(!(df$Category1 == 'NULL' | df$Category2 == 'NULL' |
          df$Category3 == 'NULL' | is.na(.)))
################################################################################

############################# zero entries #####################################
# take a look at number of zero entries
rowSums(df==0)

# remove rows with too many 0's, similar reasoning as removing Null's
df <- df[rowSums(df == 0) <= 32, ]

# remove leading and trailing 0's, 
sales <- df[, -c(1:3)]
sales <- sales[!colSums(sales) == 0]

############################# some discussion ##################################
# examine the last column (Sep.16) we can see that the sales numbers are 
# considerably lower than those of previous months (for the rows that have 
# non-zero entries), one hypothesis is that this file is recorded before the 
# end of September 2016 hence the data is incomplete and don't represent the 
# accurate sales in that month, remove??????
############################# some discussion ##################################

# remove Sep.16
sales <- select(sales, -'Sep.16')
################################################################################

# transforming data into a more plot friendly format
categories <- df[, c(1:3)]
categories <- categoryConstruction(categories)

# build final data frame
final_df <- dataFrameTransformation(sales, categories)

# extract product family starting with A from category 1
A <- final_df %>%
  select(starts_with('A'), TIME)
fam_A <- gather(A, "Product", "Sales", -TIME)

# extract product family starting with B from category 1
B <- final_df %>%
  select(starts_with('B'), TIME)
fam_B <- gather(B, "Product", "Sales", -TIME)

# extract product family starting with C from category 1
C <- final_df %>%
  select(starts_with('C'), TIME)
fam_C <- gather(C, "Product", "Sales", -TIME)

```

## EDA
This section shows some basic line charts for the data reorganized through 
Category 1, i.e. data is split into three chunks with Category 1 tags used as 
identifiers for the grouping process, so new product ID is the result of its
category 1 + category 2 + category 3 + numeric value if duplicates exist.

We can see below, first of all, each family of products has a distinct sales 
pattern and respectively products share a very similar behavior as their family
counterparts. Secondly, There is a clear annual seasonal effects in product 
family A** and C**, with both families showing a big sales surge around the end 
of each year presumably as the result of holiday shopping seasons. On the other 
hand, product family B** doesn't exhibit the same surge, instead it has a clear 
quarterly seasonal cycle throughout, with a slight hint of decreasing trend on
the two most popular products. One can postulate that the behavior of this 
product family is the result of the nature of the product, or regional effects 
such as the product is mainly circulating in markets in which holiday shopping
traditions are different.
```{r product family A** plot, echo = FALSE, message=FALSE, warning=FALSE}

ggplot(fam_A, aes(x=as.Date(TIME), y=Sales)) +
  geom_line(aes(color=Product)) + 
  xlab("") + 
  theme_minimal() + 
  scale_x_date(date_breaks = "4 month", date_labels = "%b-%Y") + 
  theme(axis.text.x=element_text(angle = 60, vjust = 0.5))

```

```{r product family B** plot, echo = FALSE, message=FALSE, warning=FALSE}

ggplot(fam_B, aes(x=as.Date(TIME), y=Sales)) +
  geom_line(aes(color=Product)) + 
  xlab("") + 
  theme_minimal() + 
  scale_x_date(date_breaks = "2 month", date_labels = "%b-%Y") + 
  theme(axis.text.x=element_text(angle = 60, vjust = 0.5))

```

```{r product family C** plot, echo = FALSE, message=FALSE, warning=FALSE}

ggplot(fam_C, aes(x=as.Date(TIME), y=Sales)) +
  geom_line(aes(color=Product)) + 
  xlab("") + 
  theme_minimal() + 
  scale_x_date(date_breaks = "4 month", date_labels = "%b-%Y") + 
  theme(axis.text.x=element_text(angle = 60, vjust = 0.5))

```


## modelling the forecasting
Since product family A** and C** exhibit similar short/long term behaviors,
one product is chosen from family A** for further analysis. Family B**'s 
distinct sales pattern is of interest hence a single product is taken from this 
family as well. The two products are chosen as the largest overall sales from 
their respective family categories.
```{r AAM, echo = FALSE, message=FALSE, warning=FALSE}
# largest sales counts from product family A**
A_name <- 
  colnames(A[-dim(A)[2]])[colSums(A[-dim(A)[2]]) == max(colSums(A[-dim(A)[2]]))]
prod_1 <- A[c(A_name, 'TIME')]

# convert to R time series
AAM <- ts(prod_1$AAM, frequency = 12, start = c(2013, 1))

# auto-correlation
Acf(AAM)
Pacf(AAM)


# clear correlations exist, hence seasonal ARIMA is a better choice
AAM_auto <- auto.arima(AAM)
checkresiduals(AAM_auto)

AAM_auto %>%
  forecast(h=12) %>%
  autoplot()

```



```{r BCW, echo = FALSE, message=FALSE, warning=FALSE}
# largest sales counts from product family B**
B_name <- 
  colnames(B[-dim(B)[2]])[colSums(B[-dim(B)[2]]) == max(colSums(B[-dim(B)[2]]))]
prod_2 <- B[c(B_name, 'TIME')]

# convert to R time series
BCW <- ts(prod_2$BCW, frequency = 12, start = c(2013, 1))
# LOG transformation
BCW <- log(BCW)

# auto-correlation
Acf(BCW)
Pacf(BCW)

# clear correlations exist, hence seasonal ARIMA is a better choice
BCW_auto <- auto.arima(BCW)
checkresiduals(BCW_auto)

BCW_auto %>%
  forecast(h=12) %>%
  autoplot()



# autocorrelation on the two product


# partial-autocorrelation to check linear dependancy

# for family A** and C**, the random and seasonal fluctuations are relatively 
# constant through time, with a hit of increasing, suggesting appropriate 
# additive model

# for family B** we have relatively constant seasonal fluctuations and with a 
# hint of decreasing trend, also suggesting a additive model

# first ac and partial ac, if yes, more complicated model, MA otherwise

#








```


# forecasting to csv

## discussions
some observations, products 

some critique on menthods, choices made, data quality

what are possible next steps, modelling cleaning etc.

