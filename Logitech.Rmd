---
title: "Logitech Interview"
author: "Chao Zhang"
date: "08/06/2020"
output: pdf_document
---

```{r setup and data prep, echo = FALSE, message=FALSE, warning=FALSE}
# load packages
library(tidyverse)
library(lubridate)
library(ggplot2)

library(caret)
library(glmnet)
library(HDCI)



library(corrplot)
library(knitr)


# set default knitr chunks
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	fig.height = 4,
	fig.pos = "H",
	fig.width = 6,
	message = FALSE,
	warning = FALSE,
	cache = FALSE
)

# source functions
source("categoryConstruction.R")
source("dataFrameTransformation.R")

# load data
df <- read.csv('use_case.csv')

```

## preprocessing and data prep
```{r EDA, echo = FALSE, message=FALSE, warning=FALSE}
# some basic data info
glimpse(df)

############################# missing values ###################################
# any missing values
sum(is.na(df))

# Yet manual inspection indicates multiple Null's
# try checking data types
sapply(df, class)

# locate rows with missing values with exact match of 'NULL' character string
nulls <- df %>%
  filter((df$Category1 == 'NULL' | df$Category2 == 'NULL' |
          df$Category3 == 'NULL' | is.na(.)))

############################# some discussion ##################################
# While the 'NULL' character strings don't necessarily have a direct impact on 
# time series analysis, missing values might indicate potential data quality 
# issues and due to the lack of extra information on the data-set removing 
# them is a safer option
############################# some discussion ##################################

# remove all rows with Null's
df <- df %>%
  filter(!(df$Category1 == 'NULL' | df$Category2 == 'NULL' |
          df$Category3 == 'NULL' | is.na(.)))
################################################################################

############################# zero entries #####################################
# take a look at number of zero entries
rowSums(df==0)

# remove rows with too many 0's, similar reasoning as removing Null's
df <- df[rowSums(df == 0) <= 32, ]

# remove leading and trailing 0's, 
sales <- df[, -c(1:3)]
sales <- sales[!colSums(sales) == 0]

############################# some discussion ##################################
# examine the last column (Sep.16) we can see that the sales numbers are 
# considerably lower than those of previous months (for the rows that have 
# non-zero entries), one hypothesis is that this file is recorded before the 
# end of September 2016 hence the data is incomplete and don't represent the 
# accurate sales in that month, remove??????
############################# some discussion ##################################

# remove Sep.16
sales <- select(sales, -'Sep.16')
################################################################################



#
categories <- df[, c(1:3)]
categories <- categoryConstruction(categories)
categories
# build final data frame
final_df <- dataFrameTransformation(sales, categories)


fam_A <- final_df %>%
  select(starts_with('A'), TIME)


fam_A <- gather(fam_A, "Product", "Sales", -TIME)

p <- ggplot(fam_A, aes(x=TIME, y=Sales)) +
  geom_line(aes(color=Product)) + 
  xlab("")
p

fam_B <- final_df %>%
  select(starts_with('B'), TIME)
fam_B <- gather(fam_B, "Product", "Sales", -TIME)

ggplot(fam_B, aes(x=TIME, y=Sales)) +
  geom_line(aes(color=Product)) + 
  xlab("")


fam_C <- final_df %>%
  select(starts_with('C'), TIME)
fam_C <- gather(fam_C, "Product", "Sales", -TIME)

ggplot(fam_B, aes(x=as.Date(TIME), y=Sales)) +
  geom_line(aes(color=Product)) + 
  xlab("") + 
  theme_minimal() + 
  scale_x_date(date_breaks = "4 month", date_labels = "%b-%Y") + 
  theme(axis.text.x=element_text(angle = 60, vjust = 0.5))


typeof(fam_C['TIME'][1


# find a way to identify 'most important products'

# maybe lowest and highest growth, biggest jumps, obvious trend? etc.


# maybe largest sell, lowest sell, most growth etc.
# some visualization on the three


df[, !(colnames(df) %in% c('Category1', 'Category2', 'Category3'))]



```



## data preparation and Methodology
Since several model selection criterions are used for this project, the data of images and voxel responses are split into three set with propotions 60 20 20. The 60% are used for training, and first 20% used to select and best model, and last 20% for () the best model's performence on unseen data. For Ridge and LASSO with different selection criterions, 10 succesive trials are ran on each combinations, this design is meant to address the issue that, according to the original study, noise that usually present in fMRI data, hence multiple bootstraps or CV's are recommanded to average out the noise. For each of the 10 runs, the following measures are recorded: significant features, average voxel-wise correlation, value of the criterion used. They are used as metrics to select the best model. Below is the brief introduction of the model selection criterions used. 

As illustrated in Table ?, we have independent variables with extremely high dimensions and in comparison, only a small set of images. The potential sparsity suggests LASSO to be a potential candidate for this analysis, and it was the main method used in the original study. Hence for the first

```{r model estimation, echo = FALSE, message=FALSE, warning=FALSE}
################################################################################
# takes long time to run (about 45min), uncomment the code below
# to load the output of this chunk from the latest run

# load("R//aic_agg.RData")
# load("R//aicc_agg.RData")
# load("R//bic_agg.RData")
# load("R//cv_agg.RData")
# load("R//escv_agg.RData")
# load("R//ridge_agg.RData")

################################################################################

#train all model combinations, save data

# train Ridge
ridge_agg <- tenFold(data = all_data, model = 'Ridge', 
                     lasso = FALSE, ic = FALSE)
save(ridge_agg, file = "R//ridge.RData")

# train LASSO with CV and ESCV
cv_agg <- tenFold(data = all_data, model = 'CV', 
                  lasso = TRUE, ic = FALSE)
save(cv_agg, file = "R//cv_agg.RData")

escv_agg <- tenFold(data = all_data, model = 'ESCV', 
                    lasso = TRUE, ic = FALSE)
save(escv_agg, file = "R//escv_agg.RData")

# train LASSO with AIC, AICc and BIC
aic_agg <- tenFold(data = all_data, model = 'min_AIC', 
                   lasso = TRUE, ic = TRUE)
save(aic_agg, file = "R//aic_agg.RData")

aicc_agg <- tenFold(data = all_data, model = 'min_AICc', 
                    lasso = TRUE, ic = TRUE)
save(aicc_agg, file = "R//aicc_agg.RData")

bic_agg <- tenFold(data = all_data, model = 'min_BIC', 
                   lasso = TRUE, ic = TRUE)
save(bic_agg, file = "R//bic_agg.RData")

```

```{r extract info, echo = FALSE, message=FALSE, warning=FALSE}
# functions
corrVoxel <- function(obj) {
  # function to extract correlations, per voxel
  m <- matrix(nrow = 10, ncol = 20)
  #
  for (i in 1:10) { # per trial
    temp <- obj[[10 + i]]
    for (j in 1:20) { # per voxel
      m[i, j] <- temp[[j]]
    }
  }
  return(m)
}

sigVar <- function(obj) {
  # function to extract significant features count
  var <- c()
  dim2 <- dim(all_data[['test_X']])[2]
  #
  for (i in 1:20) { # per voxel
    voxels <- matrix(nrow = 10, ncol = dim2)
    for (j in 1:10) { # per trial
      voxels[j, ] <- (as.vector(obj[[j]][[i]]$beta) != 0)
    }
    var[i] <- sum((colSums(voxels) / 10) > 0.8)
  }
  return(var)
}

sigFea <- function(obj) {
  # function to extract significant features id's
  var <- list()
  dim2 <- dim(all_data[['test_X']])[2]
  #
  for (i in 1:20) { # per voxel
    voxels <- matrix(nrow = 10, ncol = dim2)
    for (j in 1:10) { # per trial
      voxels[j, ] <- (as.vector(obj[[j]][[i]]$beta) != 0)
    }
    var[[i]] <- (colSums(voxels))
  }
  return(var)
}

# save useful information
################################################################################
# correlation voxel mean
escv_corr_mean <- colMeans(corrVoxel(escv_agg))
cv_corr_mean <- colMeans(corrVoxel(cv_agg))
aic_corr_mean <- colMeans(corrVoxel(aic_agg))
aicc_corr_mean <- colMeans(corrVoxel(aicc_agg))
bic_corr_mean <- colMeans(corrVoxel(bic_agg))
ridge_corr_mean <- colMeans(corrVoxel(ridge_agg))

# correlation voxel standard deviation
escv_corr_std <- apply(corrVoxel(escv_agg), MARGIN = 2, FUN = sd)
cv_corr_std <- apply(corrVoxel(cv_agg), MARGIN = 2, FUN = sd)
aic_corr_std <- apply(corrVoxel(aic_agg), MARGIN = 2, FUN = sd)
aicc_corr_std <- apply(corrVoxel(aicc_agg), MARGIN = 2, FUN = sd)
bic_corr_std <- apply(corrVoxel(bic_agg), MARGIN = 2, FUN = sd)
ridge_corr_std <- apply(corrVoxel(ridge_agg), MARGIN = 2, FUN = sd)

# count of significant features that appeared with probability at least 0.8, per voxel, per model
escv_count <- sigVar(escv_agg)
cv_count <- sigVar(cv_agg)
aic_count <- sigVar(aic_agg)
aicc_count <- sigVar(aicc_agg)
bic_count <- sigVar(bic_agg)
ridge_count <- sigVar(ridge_agg)
################################################################################

```

# results
To choose the best model for each voxel, three criterions are used: average voxel-wise accuracy, voxel-wise correlation standard deviation and significant features counts. Below Table 2 shows the voxel-wise correlation. We can see that, firstly there are many NA values especially in the column of BIC, this is caused by the best model chosen under that criterion, in at least 1 of the 10 trials, only selects the intercept as independent variable, hence all predicted values are the same scalar and correlation with observed values can not be calculated. At the same time, even though for many voxels Ridge is albe to provide similar prediction correlations compare to other models, the fact that it does not offer feature selection makes it less attractive for this project. And we see that AICc offers the worst result. Thus, moving forward LASSO with AICc, BIC and Ridge is disregarded.

```{r voxel-wise correlation, echo = FALSE, message=FALSE, warning=FALSE}
# results
################################################################################
# correlations, all voxels
all_corr <- data.frame('Voxel' = 1:20, 'ESCV' = escv_corr_mean, 
                       'CV' = cv_corr_mean, 'AIC' = aic_corr_mean, 
                       'AICc' = aicc_corr_mean, 'BIC' = bic_corr_mean, 
                       'Ridge' = ridge_corr_mean)


kable_styling(kable(all_corr), full_width = FALSE)
################################################################################

```
Table 2

Next, the other two metrics: voxel-wise correlations standard deviation and significant features count are illustrated in Table 3 below, for all 20 voxels:

```{r other two metrics, echo = FALSE, message=FALSE, warning=FALSE}
# results 
################################################################################
# all metrics, all voxels
all_other_metrics <- data.frame('Voxel' = 1:20, 'ESCV_STD' = escv_corr_std,
                       'ESCV_COUNT' = escv_count, 'CV_STD' = cv_corr_std,
                       'CV_COUNT' = cv_count, 'AIC_STD' = aic_corr_std,
                       'AIC_COUNT' = aic_count)


kable_styling(kable(all_other_metrics), full_width = FALSE)
################################################################################

```
Table 3

## goodness of fit
From the results above we can see that, for almost all models, the only feature chosen on voxel 20 is the intercept (which explains why there is NA in the correlations and relative fields). This suggest that either the shrinkage parameters range does not include the optimal values thus the models can not find the minimum, or there is no predictive power on voxel 20 at all based on the data at hand. To further investigate, diagnostic plots are generated for voxel 20, and the same is done on voxel 9 to compare, which is the the voxel has the highest predicted correlation. Below in Figure ? are some diagnostic plots of the two models: 
```{r goodness of fit, echo = FALSE, message=FALSE, warning=FALSE}
# diagnostic and residuals plot, on voxel 9 and 11

# for CV path plot
cv_9 <- cv.glmnet(x = all_data[['train_X']], y = all_data[['train_y']][, 9], 
                    family = 'gaussian', alpha = 1, nfolds = 5, parallel = TRUE)
cv_11 <- cv.glmnet(x = all_data[['train_X']], y = all_data[['train_y']][, 11], 
                    family = 'gaussian', alpha = 1, nfolds = 5, parallel = TRUE)
cv_20 <- cv.glmnet(x = all_data[['train_X']], y = all_data[['train_y']][, 20], 
                    family = 'gaussian', alpha = 1, nfolds = 5, parallel = TRUE)

# ESCV on voxel 9 and 11
escv_lambda <- glmModelSelection(data = all_data, voxel = voxel, lasso = TRUE, ic = FALSE)

escv_9 <- glmnet(x = all_data[['train_X']], y = all_data[['train_y']][, 9], 
                          alpha = 1, family = 'gaussian', 
                          lambda = escv_lambda[['ESCV_Lambda']][[9]])
escv_11 <- glmnet(x = all_data[['train_X']], y = all_data[['train_y']][, 11], 
                          alpha = 1, type.measure = 'mse', family = 'gaussian', 
                          lambda = escv_lambda[['ESCV_Lambda']][[11]])
 
# AIC on voxel 9 and 11
aic_lambda <- glmModelSelection(data = all_data, voxel = voxel, lasso = TRUE, ic = TRUE)

aic_9 <- glmnet(x = all_data[['train_X']], y = all_data[['train_y']][, 9], 
                    family = 'gaussian', alpha = 1, 
                    lambda = aic_lambda[['min_AIC_Lambda']][[9]])

aic_11 <- glmnet(x = all_data[['train_X']], y = all_data[['train_y']][, 9], 
                    family = 'gaussian', alpha = 1, 
                    lambda = aic_lambda[['min_AIC_Lambda']][[9]])

# residual calculation
escv_9_pred <- predict(object = escv_9, newx = all_data[['train_X']])
escv_11_pred <- predict(object = escv_11, newx = all_data[['train_X']])

escv_9_resid <- escv_9_pred - all_data[['train_y']][, 9]
escv_11_resid <- escv_11_pred - all_data[['train_y']][, 11]

aic_9_pred <- predict(object = aic_9, newx = all_data[['train_X']])
aic_11_pred <- predict(object = aic_11, newx = all_data[['train_X']])

aic_9_resid <- aic_9_pred - all_data[['train_y']][, 9]
aic_11_resid <- aic_11_pred - all_data[['train_y']][, 11]

df_9_escv <- data.frame(escv_9_pred, escv_9_resid)
df_11_escv <- data.frame(escv_11_pred, escv_11_resid)

df_9_aic <- data.frame(aic_9_pred, aic_9_resid)
df_11_aic <- data.frame(aic_11_pred, aic_11_resid)

# residuals plots of the best and worst predicted voxels
ggplot(df_9_escv, aes(x=s0, y=s0.1)) +
  geom_point(size=1, shape=23) + 
  geom_smooth(method=lm) + labs(title = "Residual plot for voxel 9 (ESCV)", 
                                x = "Residuals", y = "Predicted")

ggplot(df_11_escv, aes(x=s0, y=s0.1)) +
  geom_point(size=1, shape=23) + 
  geom_smooth(method=lm) + labs(title = "Residual plot for voxel 11 (ESCV)", 
                                x = "Residuals", y = "Predicted")

ggplot(df_9_aic, aes(x=s0, y=s0.1)) +
  geom_point(size=1, shape=23) + 
  geom_smooth(method=lm) + labs(title = "Residual plot for voxel 9 (AIC)", 
                                x = "Residuals", y = "Predicted")

ggplot(df_11_aic, aes(x=s0, y=s0.1)) +
  geom_point(size=1, shape=23) + 
  geom_smooth(method=lm) + labs(title = "Residual plot for voxel 11 (AIC)", 
                                x = "Residuals", y = "Predicted")

# CV plots for voxel 9 and 20
plot(cv_20)

plot(cv_9)

```

From the above plots we can see that as the shrinkage parameter increases, the loss of voxel 20 does not increases, unlike the loss from voxel 9. Furthermore, its loss keeps decreasing till the model only selects the intercept, and it is not reasonable to speculate that the dependent variables hold no predictive powers at all on voxel 20. On the contrarary, we see the loss of voxel 9 decreases, then increases later as lambda getting larger, which suggests a minimum value found through lambda's path.
This helps explaining why almost all models have NA in voxel 20, hence it should be excluded from considerations for choosing the best model later on.
Next, goodness of fit of the model is explored and below in Figure ? is the residual plot for voxel 9 and 11 respectively: the best and worst performing voxel with respect to correlation:

Figure ?

We see that for both voxel the residual plots suggest a pretty good fit on LASSO, and it is still inconclusive whether one is better than the other at this stage. To further investigate the two models, let us take a look at the features selected by the two:


```{r model diagnostics, echo = FALSE, message=FALSE, warning=FALSE}
# count features selected for each voxel
escv_var <- sigFea(escv_agg)
aic_var <- sigFea(aic_agg)

inte <- list()
aic_sig <- c()
escv_sig <- c()
for (i in 1:20) {
  inte[[i]] <- intersect(which(escv_var[[i]] == TRUE), which(aic_var[[i]] == TRUE))
  aic_sig[i] <- length(which(aic_var[[i]] == TRUE))
  escv_sig[i] <- length(which(escv_var[[i]] == TRUE))
}

counts <- t(data.frame('voxel' = 1:20, aic_sig, escv_sig))
kable_styling(kable(counts), full_width = FALSE)


```

Table 4 above show the total variables used by ESCV and AIC for all voxels. We see that in general ESCV selects less variables compare to AIC while still maintains very similar prediciton correlations. Since AIC requires additional model assumptions, the model-free criterion ESCV is faviroble for this application, thus, the final model selected is LASSO with ESCV.

## discussion and conclusion
Below Figure 3 shows the correlation on the test set:
```{r best model on test set, echo = FALSE, message=FALSE, warning=FALSE}
# final model for prediction on the test set
final_model_params <- glmModelConstruction(data = all_data, 
                          lambdas = escv_lambda[['ESCV_Lambda']],
                          lasso = TRUE, test = TRUE)
final_corr <- c()
for (i in 1:20) {
  final_corr[i] <- final_model_params[[2]][[i]]
}
final_corr <- t(data.frame('voxel' = sapply(1:20, FUN = toString), final_corr))
kable_styling(kable(final_corr), full_width = FALSE, font_size = 13)

```
Figure 3

Comparing with the stage of model selection, most voxels see a moderate improvement on correlation when the training data is increased in size. At the same time, now only 2 models, voxel 16 and 20, select only the intercept, which is better than 4 models in previous stage.  


```{r predict the final test set, echo = FALSE, message=FALSE, warning=FALSE}
# model training on the entire dataset
whole_model_params <- escv.glmnet(x = fit_feat, y = resp_dat[, 1], 
                                  nfolds = 10, parallel = TRUE, alpha = 1,
                                  family = 'gaussian')

whole_model <- glmnet(x = fit_feat, y = resp_dat[, 1], 
                      family = 'gaussian', alpha = 1, 
                      lambda = whole_model_params$lambda.escv)

# prediction on the test set
val_feat_pred <- predict(whole_model, val_feat)
# save test results into a .txt file
write.table(val_feat_pred, file = "output//predv1_chaozhang.txt", 
            append = FALSE, sep = " ", dec = ".",
            row.names = FALSE, col.names = FALSE)

```
