---
title: "Logitech Interview"
author: "Chao Zhang"
date: "08/06/2020"
output: pdf_document
---

```{r setup and data prep, echo = FALSE, message=FALSE, warning=FALSE}
# load packages
library(tidyverse)
library(lubridate)
library(ggplot2)

library(caret)
library(glmnet)
library(HDCI)



library(corrplot)
library(knitr)


# set default knitr chunks
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	fig.height = 4,
	fig.pos = "H",
	fig.width = 6,
	message = FALSE,
	warning = FALSE,
	cache = FALSE
)

# source functions
source("categoryConstruction.R")
source("dataFrameTransformation.R")

# load data
df <- read.csv('use_case.csv')

```



## preprocessing and data prep
```{r EDA, echo = FALSE, message=FALSE, warning=FALSE}
# some basic data info
glimpse(df)

############################# missing values ###################################
# any missing values
sum(is.na(df))

# Yet manual inspection indicates multiple Null's
# try checking data types
sapply(df, class)

# locate rows with missing values with exact match of 'NULL' character string
nulls <- df %>%
  filter((df$Category1 == 'NULL' | df$Category2 == 'NULL' |
          df$Category3 == 'NULL' | is.na(.)))

############################# some discussion ##################################
# While the 'NULL' character strings don't necessarily have a direct impact on 
# time series analysis, missing values might indicate potential data quality 
# issues and due to the lack of extra information on the data-set removing 
# them is a safer option
############################# some discussion ##################################

# remove all rows with Null's
df <- df %>%
  filter(!(df$Category1 == 'NULL' | df$Category2 == 'NULL' |
          df$Category3 == 'NULL' | is.na(.)))
################################################################################

############################# zero entries #####################################
# take a look at number of zero entries
rowSums(df==0)

# remove rows with too many 0's, similar reasoning as removing Null's
df <- df[rowSums(df == 0) <= 32, ]

# remove leading and trailing 0's, 
sales <- df[, -c(1:3)]
sales <- sales[!colSums(sales) == 0]

############################# some discussion ##################################
# examine the last column (Sep.16) we can see that the sales numbers are 
# considerably lower than those of previous months (for the rows that have 
# non-zero entries), one hypothesis is that this file is recorded before the 
# end of September 2016 hence the data is incomplete and don't represent the 
# accurate sales in that month, remove??????
############################# some discussion ##################################

# remove Sep.16
sales <- select(sales, -'Sep.16')
################################################################################

# transforming data into a more plot friendly format
categories <- df[, c(1:3)]
categories <- categoryConstruction(categories)
categories
# build final data frame
final_df <- dataFrameTransformation(sales, categories)

# extract product family starting with A from category 1
fam_A <- final_df %>%
  select(starts_with('A'), TIME)
fam_A <- gather(fam_A, "Product", "Sales", -TIME)

# extract product family starting with B from category 1
fam_B <- final_df %>%
  select(starts_with('B'), TIME)
fam_B <- gather(fam_B, "Product", "Sales", -TIME)

# extract product family starting with C from category 1
fam_C <- final_df %>%
  select(starts_with('C'), TIME)
fam_C <- gather(fam_C, "Product", "Sales", -TIME)


# find a way to identify 'most important products'

# maybe lowest and highest growth, biggest jumps, obvious trend? etc.


# maybe largest sell, lowest sell, most growth etc.
# some visualization on the three




```

## some EDA plots
we can see that seasonal affects, holiday shopping seasons, while product 
family B doesn't exhibite the same cyclicle, maybe due to regional or nature 
of the product it has a more consistent sales cycle etc.
```{r product family A** plot, echo = FALSE, message=FALSE, warning=FALSE}

ggplot(fam_A, aes(x=as.Date(TIME), y=Sales)) +
  geom_line(aes(color=Product)) + 
  xlab("") + 
  theme_minimal() + 
  scale_x_date(date_breaks = "4 month", date_labels = "%b-%Y") + 
  theme(axis.text.x=element_text(angle = 60, vjust = 0.5))

```

```{r product family B** plot, echo = FALSE, message=FALSE, warning=FALSE}

ggplot(fam_B, aes(x=as.Date(TIME), y=Sales)) +
  geom_line(aes(color=Product)) + 
  xlab("") + 
  theme_minimal() + 
  scale_x_date(date_breaks = "4 month", date_labels = "%b-%Y") + 
  theme(axis.text.x=element_text(angle = 60, vjust = 0.5))

```

```{r product family C** plot, echo = FALSE, message=FALSE, warning=FALSE}

ggplot(fam_C, aes(x=as.Date(TIME), y=Sales)) +
  geom_line(aes(color=Product)) + 
  xlab("") + 
  theme_minimal() + 
  scale_x_date(date_breaks = "4 month", date_labels = "%b-%Y") + 
  theme(axis.text.x=element_text(angle = 60, vjust = 0.5))

```

## data preparation and Methodology
Since product family A** and C** both exhibites similar long term behaviors, 
for the interest of time and lack of info, only use one. Here choosing the 
one with highest overall sales from family A**. On the other hand, product 
family B** has totally different behaviors with short term sensonality and 
slightly decreasing trend for the one with highest sales, hence chosen to be 
the interest of analysis.

```{r model estimation, echo = FALSE, message=FALSE, warning=FALSE}

# autocorrelation on the two product


# partial-autocorrelation to check linear dependancy

# for family A** and C**, the random and seasonal fluctuations are relatively 
# constant through time, with a hit of increasing, suggesting appropriate 
# additive model

# for family B** we have relatively constant seasonal fluctuations and with a 
# hint of decreasing trend, also suggesting a additive model

# first ac and partial ac, if yes, more complicated model, MA otherwise

# 










################################################################################
# takes long time to run (about 45min), uncomment the code below
# to load the output of this chunk from the latest run

# load("R//aic_agg.RData")
# load("R//aicc_agg.RData")
# load("R//bic_agg.RData")
# load("R//cv_agg.RData")
# load("R//escv_agg.RData")
# load("R//ridge_agg.RData")

################################################################################

#train all model combinations, save data

# train Ridge
ridge_agg <- tenFold(data = all_data, model = 'Ridge', 
                     lasso = FALSE, ic = FALSE)
save(ridge_agg, file = "R//ridge.RData")

# train LASSO with CV and ESCV
cv_agg <- tenFold(data = all_data, model = 'CV', 
                  lasso = TRUE, ic = FALSE)
save(cv_agg, file = "R//cv_agg.RData")

escv_agg <- tenFold(data = all_data, model = 'ESCV', 
                    lasso = TRUE, ic = FALSE)
save(escv_agg, file = "R//escv_agg.RData")

# train LASSO with AIC, AICc and BIC
aic_agg <- tenFold(data = all_data, model = 'min_AIC', 
                   lasso = TRUE, ic = TRUE)
save(aic_agg, file = "R//aic_agg.RData")

aicc_agg <- tenFold(data = all_data, model = 'min_AICc', 
                    lasso = TRUE, ic = TRUE)
save(aicc_agg, file = "R//aicc_agg.RData")

bic_agg <- tenFold(data = all_data, model = 'min_BIC', 
                   lasso = TRUE, ic = TRUE)
save(bic_agg, file = "R//bic_agg.RData")

```

```{r extract info, echo = FALSE, message=FALSE, warning=FALSE}
# functions
corrVoxel <- function(obj) {
  # function to extract correlations, per voxel
  m <- matrix(nrow = 10, ncol = 20)
  #
  for (i in 1:10) { # per trial
    temp <- obj[[10 + i]]
    for (j in 1:20) { # per voxel
      m[i, j] <- temp[[j]]
    }
  }
  return(m)
}

sigVar <- function(obj) {
  # function to extract significant features count
  var <- c()
  dim2 <- dim(all_data[['test_X']])[2]
  #
  for (i in 1:20) { # per voxel
    voxels <- matrix(nrow = 10, ncol = dim2)
    for (j in 1:10) { # per trial
      voxels[j, ] <- (as.vector(obj[[j]][[i]]$beta) != 0)
    }
    var[i] <- sum((colSums(voxels) / 10) > 0.8)
  }
  return(var)
}

sigFea <- function(obj) {
  # function to extract significant features id's
  var <- list()
  dim2 <- dim(all_data[['test_X']])[2]
  #
  for (i in 1:20) { # per voxel
    voxels <- matrix(nrow = 10, ncol = dim2)
    for (j in 1:10) { # per trial
      voxels[j, ] <- (as.vector(obj[[j]][[i]]$beta) != 0)
    }
    var[[i]] <- (colSums(voxels))
  }
  return(var)
}

# save useful information
################################################################################
# correlation voxel mean
escv_corr_mean <- colMeans(corrVoxel(escv_agg))
cv_corr_mean <- colMeans(corrVoxel(cv_agg))
aic_corr_mean <- colMeans(corrVoxel(aic_agg))
aicc_corr_mean <- colMeans(corrVoxel(aicc_agg))
bic_corr_mean <- colMeans(corrVoxel(bic_agg))
ridge_corr_mean <- colMeans(corrVoxel(ridge_agg))

# correlation voxel standard deviation
escv_corr_std <- apply(corrVoxel(escv_agg), MARGIN = 2, FUN = sd)
cv_corr_std <- apply(corrVoxel(cv_agg), MARGIN = 2, FUN = sd)
aic_corr_std <- apply(corrVoxel(aic_agg), MARGIN = 2, FUN = sd)
aicc_corr_std <- apply(corrVoxel(aicc_agg), MARGIN = 2, FUN = sd)
bic_corr_std <- apply(corrVoxel(bic_agg), MARGIN = 2, FUN = sd)
ridge_corr_std <- apply(corrVoxel(ridge_agg), MARGIN = 2, FUN = sd)

# count of significant features that appeared with probability at least 0.8, per voxel, per model
escv_count <- sigVar(escv_agg)
cv_count <- sigVar(cv_agg)
aic_count <- sigVar(aic_agg)
aicc_count <- sigVar(aicc_agg)
bic_count <- sigVar(bic_agg)
ridge_count <- sigVar(ridge_agg)
################################################################################

```

# results
To choose the best model for each voxel, three criterions are used: average voxel-wise accuracy, voxel-wise correlation standard deviation and significant features counts. Below Table 2 shows the voxel-wise correlation. We can see that, firstly there are many NA values especially in the column of BIC, this is caused by the best model chosen under that criterion, in at least 1 of the 10 trials, only selects the intercept as independent variable, hence all predicted values are the same scalar and correlation with observed values can not be calculated. At the same time, even though for many voxels Ridge is albe to provide similar prediction correlations compare to other models, the fact that it does not offer feature selection makes it less attractive for this project. And we see that AICc offers the worst result. Thus, moving forward LASSO with AICc, BIC and Ridge is disregarded.

