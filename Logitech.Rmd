---
title: "Logitech Interview"
author: "Chao Zhang"
date: "08/06/2020"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup and data prep, echo = FALSE, message=FALSE, warning=FALSE}
# load packages
library(tidyverse)
library(lubridate)
library(ggplot2)
library(forecast)
library(zoo)
library(knitr)

# set default knitr chunks
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	fig.height = 4,
	fig.pos = "H",
	fig.width = 6,
	message = FALSE,
	warning = FALSE,
	cache = FALSE
)

# source functions
source("categoryConstruction.R")
source("dataFrameTransformation.R")

# load data
df <- read.csv('use_case.csv')

```

## preprocessing and data prep
This section focuses on preprocessing and cleaning the data. Some 
justifications on the decisions made are provided in the comments.
And outputs are supressed for this chunk.
```{r EDA, echo = FALSE, message=FALSE, warning=FALSE, include=FALSE}
# some basic data info
glimpse(df)

############################# missing values ###################################
# any missing values
sum(is.na(df))

# Yet manual inspection indicates multiple Null's
# try checking data types
sapply(df, class)

# locate rows with missing values with exact match of 'NULL' character string
nulls <- df %>%
  filter((df$Category1 == 'NULL' | df$Category2 == 'NULL' |
          df$Category3 == 'NULL' | is.na(.)))

############################# some discussion ##################################
# While the 'NULL' character strings don't necessarily have a direct impact on 
# time series analysis, missing values might indicate potential data quality 
# issues and due to the lack of extra information on the data-set removing 
# them is a safer option
############################# some discussion ##################################

# remove all rows with Null's
df <- df %>%
  filter(!(df$Category1 == 'NULL' | df$Category2 == 'NULL' |
          df$Category3 == 'NULL' | is.na(.)))
################################################################################

############################# zero entries #####################################
# take a look at number of zero entries
rowSums(df==0)

# remove rows with too many 0's, similar reasoning as removing Null's
df <- df[rowSums(df == 0) <= 32, ]

# remove leading and trailing 0's, 
sales <- df[, -c(1:3)]
sales <- sales[!colSums(sales) == 0]

############################# some discussion ##################################
# examine the last column (Sep.16) we can see that the sales numbers are 
# considerably lower than those of previous months (for the rows that have 
# non-zero entries), one hypothesis is that this file is recorded before the 
# end of September 2016 hence the data is incomplete and don't represent the 
# accurate sales in that month, remove??????
############################# some discussion ##################################

# remove Sep.16
sales <- select(sales, -'Sep.16')
################################################################################

# transforming data into a more plot friendly format
categories <- df[, c(1:3)]
categories <- categoryConstruction(categories)

# build final data frame
final_df <- dataFrameTransformation(sales, categories)

# extract product family starting with A from category 1
A <- final_df %>%
  select(starts_with('A'), TIME)
fam_A <- gather(A, "Product", "Sales", -TIME)

# extract product family starting with B from category 1
B <- final_df %>%
  select(starts_with('B'), TIME)
fam_B <- gather(B, "Product", "Sales", -TIME)

# extract product family starting with C from category 1
C <- final_df %>%
  select(starts_with('C'), TIME)
fam_C <- gather(C, "Product", "Sales", -TIME)

```

## EDA
This section shows some basic line charts for the data reorganized through 
Category 1, i.e. data is split into three chunks with Category 1 tags used as 
identifiers for the grouping process, so new product ID is the result of its
category 1 + category 2 + category 3 + numeric value if duplicates exist.

We can see below, first of all, each family of products has a distinct sales 
pattern and respectively products share a very similar behavior as their family
counterparts. Secondly, There is a clear annual seasonal effect in product 
family A** and C**, with both families showing a big sales surge around the end 
of each year presumably as the result of holiday shopping seasons. On the other 
hand, product family B** mostly doesn't exhibit the same surge, instead it has 
a clear quarterly seasonal cycle throughout, with a slight hint of decreasing 
trend on the two most popular products. One can postulate that the behavior of 
this product family is the result of the nature of the product, or regional 
effects such as the product is mainly circulating in markets in which holiday 
shopping traditions are different.
```{r product family A** plot, echo = FALSE, message=FALSE, warning=FALSE}

ggplot(fam_A, aes(x=as.Date(TIME), y=Sales)) +
  geom_line(aes(color=Product)) + 
  xlab("") + 
  theme_minimal() + 
  scale_x_date(date_breaks = "4 month", date_labels = "%b-%Y") + 
  theme(axis.text.x=element_text(angle = 60, vjust = 0.5))

```

```{r product family B** plot, echo = FALSE, message=FALSE, warning=FALSE}

ggplot(fam_B, aes(x=as.Date(TIME), y=Sales)) +
  geom_line(aes(color=Product)) + 
  xlab("") + 
  theme_minimal() + 
  scale_x_date(date_breaks = "2 month", date_labels = "%b-%Y") + 
  theme(axis.text.x=element_text(angle = 60, vjust = 0.5))

```

```{r product family C** plot, echo = FALSE, message=FALSE, warning=FALSE}

ggplot(fam_C, aes(x=as.Date(TIME), y=Sales)) +
  geom_line(aes(color=Product)) + 
  xlab("") + 
  theme_minimal() + 
  scale_x_date(date_breaks = "4 month", date_labels = "%b-%Y") + 
  theme(axis.text.x=element_text(angle = 60, vjust = 0.5))

```


## modelling the forecasting
Since product family A** and C** exhibit similar short/long term behaviors,
one product is chosen from family A** for further analysis. Family B**'s 
distinct sales pattern is of interest hence a single product is taken from this 
family as well. The two products are chosen as the largest overall sales from 
their respective family categories.
```{r AAM, echo = FALSE, message=FALSE, warning=FALSE}
# largest sales counts from product family A**
A_name <- 
  colnames(A[-dim(A)[2]])[colSums(A[-dim(A)[2]]) == max(colSums(A[-dim(A)[2]]))]
prod_1 <- A[c(A_name, 'TIME')]

# convert to R time series
AAM <- ts(prod_1$AAM, frequency = 12, start = c(2013, 1))

# auto-correlation
Acf(AAM)
Pacf(AAM)

# SARIMA to account for seasonality and correlation
AAM_auto <- auto.arima(AAM)
checkresiduals(AAM_auto)

# 12 months prediction
AAM_pred <- AAM_auto %>%
  forecast(h=12) 

# prediction plot
AAM_pred %>%
  autoplot()

# summary of model
summary(AAM_pred)

```

```{r BCW, echo = FALSE, message=FALSE, warning=FALSE}
# largest sales counts from product family B**
B_name <- 
  colnames(B[-dim(B)[2]])[colSums(B[-dim(B)[2]]) == max(colSums(B[-dim(B)[2]]))]
prod_2 <- B[c(B_name, 'TIME')]

# convert to R time series
BCW <- ts(prod_2$BCW, frequency = 12, start = c(2013, 1))
# LOG transformation for the seemingly non-linear decreasing trend
BCW <- log(BCW)

# auto-correlation
Acf(BCW)
Pacf(BCW)

# seasonal ARIMA to account for seasonality and correlation
BCW_auto <- auto.arima(BCW)
checkresiduals(BCW_auto)

# 12 months forecast
BCW_pred <- BCW_auto %>%
  forecast(h=12) 

# prediction plot
BCW_pred%>%
  autoplot()

# summary of the model
summary(BCW_pred)

```

## save forecasting to csv
```{r save prediction}

AAM_pred_series <- ts(c(AAM_pred$fitted, AAM_pred$mean), start = start(AAM_pred$fitted), 
   frequency = frequency(AAM_pred$fitted))
BCW_pred_series <- ts(c(BCW_pred$fitted, BCW_pred$mean), start = start(BCW_pred$fitted), 
   frequency = frequency(BCW_pred$fitted))

final_pred <- data.frame('TIME' = as.yearmon(time(AAM_pred_series)),
            'AAM' = as.matrix(AAM_pred_series), 
            'BCW (log)' = as.matrix(BCW_pred_series))

# save to csv
write.csv(final_pred, "prediction.csv")

final_pred

```

## discussions
Preprocessing and cleaning: 
  The cleaning process takes a more conservative approach and removes all the 
  observations with 'NULL''s and too many 0's, under the assumption that they
  may be a sign of data quality issues. If more information is available for
  the data-set, such as domain knowledge and how the data is collected and 
  stored, the cleaning process may be different.

EDA:
  The goal of reorganizing the data in such a way above is trying to gain more
  insights on different product families. Indeed products in the same category 
  share very similar sales patterns for all three major families. 
  Another interesting observations is a slight hint or sales convergence in 
  product family B, as we can see a trend of increasing for historically lower
  sales products and a trend of decreasing for the ones with highest sales. This 
  can be of interests of further analysis with more information on the products
  and the market.
  Over plotting is an issues in this section as the majority of products in each
  family have similar sales numbers hence the lines are all clustered together,
  yet the plot is still useful to identify product family characteristics and 
  overall trend.
  
Modeling:
  The two products selected for modeling are the ones with highest overall sales 
  from family A and B. The reasons behind such choices are firstly, family
  A and C shares a very similar short and long term sales patterns, without 
  extra knowledge on the products the process of finding a suitable model for 
  the two families will be nearly identical, hence no product is selected from 
  C; secondly, family A and B have very different behaviors in terms of 
  seasonality and trend, which may represent different product nature and 
  other market affects; and lastly the highest sales of each category may 
  present insights of the market with some analysis, hence they are suitable 
  for the first step of the data analytic process.
  For the modeling process this document only presents the very first step of
  forecasting. More in depth investigation in terms of data statistical 
  properties and explorations for stationary and seasonality transformation 
  are needed for fine tuning the model in the future. Seasonal ARIMA model is 
  chosen to account for seasonality and the high correlations observed in the 
  data, and auto.arima is a good starting benchmark for future tuning. 
  